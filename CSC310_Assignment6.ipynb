{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ojCrjUQ4-3bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71fc604-d379-4d9a-839a-0f193e4fb442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Age          0\n",
            "Gender       0\n",
            "TB           0\n",
            "DB           0\n",
            "Alkphos      0\n",
            "sgpt         0\n",
            "sgot         0\n",
            "TP           0\n",
            "ALB          0\n",
            "A/G ratio    4\n",
            "Selector     0\n",
            "dtype: int64\n",
            "Model: DecisionTreeClassifier(max_depth=1), Train Accuracy: 0.7060085836909872, Test Accuracy: 0.7435897435897436\n",
            "Model: DecisionTreeClassifier(max_depth=5), Train Accuracy: 0.776824034334764, Test Accuracy: 0.7435897435897436\n",
            "Model: DecisionTreeClassifier(), Train Accuracy: 1.0, Test Accuracy: 0.7435897435897436\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/IndianLiver.csv'\n",
        "column_names = ['Age', 'Gender', 'TB', 'DB', 'Alkphos', 'sgpt', 'sgot', 'TP', 'ALB', 'A/G ratio', 'Selector']\n",
        "data = pd.read_csv(file_path, header=None, names=column_names)\n",
        "\n",
        "# Checking for NaN values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Handling NaN values for numerical columns\n",
        "numerical_cols = ['Age', 'TB', 'DB', 'Alkphos', 'sgpt', 'sgot', 'TP', 'ALB', 'A/G ratio']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
        "\n",
        "# If 'Gender' had NaN values or special handling was needed:\n",
        "# data['Gender'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Convert 'Gender' to a binary variable (ensure no NaN values beforehand)\n",
        "data['Gender'] = pd.get_dummies(data['Gender'], drop_first=True)\n",
        "\n",
        "# Split the dataset into features (X) and the label (y)\n",
        "X = data.drop('Selector', axis=1)\n",
        "y = data['Selector']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create decision tree models with varying depths\n",
        "models = [DecisionTreeClassifier(max_depth=1),  # Low complexity\n",
        "          DecisionTreeClassifier(max_depth=5),  # Medium complexity\n",
        "          DecisionTreeClassifier(max_depth=None)]  # High complexity\n",
        "\n",
        "# Fit models and evaluate their accuracy\n",
        "for model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    print(f\"Model: {model}, Train Accuracy: {accuracy_score(y_train, y_pred_train)}, Test Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Imputing missing values in 'A/G ratio'\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "data['A/G ratio'] = imputer.fit_transform(data[['A/G ratio']])\n",
        "\n",
        "# Parameters to be tested in the grid search\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': range(1, 11)  # Testing depths from 1 to 10\n",
        "}\n",
        "\n",
        "# Decision tree classifier for the grid search\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Score: {best_score}\")\n",
        "\n",
        "# Best model according to grid search\n",
        "best_model = DecisionTreeClassifier(criterion=best_params['criterion'], max_depth=best_params['max_depth'])\n",
        "\n",
        "# Perform cross-validation to get a new set of scores for the best model\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
        "\n",
        "# Compute mean and standard deviation of the new scores\n",
        "mean_score = np.mean(cv_scores)\n",
        "std_dev = np.std(cv_scores, ddof=1)\n",
        "\n",
        "# Compute 95% Confidence Interval for the mean score\n",
        "confidence_interval = (mean_score - 1.96 * std_dev, mean_score + 1.96 * std_dev)\n",
        "\n",
        "print(f\"95% Confidence Interval: {confidence_interval}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn6QubWkRcEj",
        "outputId": "e5199f9a-0c0b-4b3c-fe67-2fa0eccf4dfd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'criterion': 'gini', 'max_depth': 1}\n",
            "Best Cross-Validation Score: 0.7060169297643561\n",
            "95% Confidence Interval: (0.6959451585923992, 0.716088700936313)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low Comp Model\n",
        "The low complexity model had a training accuracy of approximately 70.6% and a test accuracy of approximately 74.4%. It scores decently in both training and testing, suggesting a balance between learning and generalizing to new data.\n",
        "\n",
        "# Medium Comp Model\n",
        "The medium complexity model improved the training accuracy to about 77.7% without changing the test accuracy, indicating a better fit to the training data while maintaining performance. It improves on training accuracy but doesn't see a boost in testing performance, indicating a better fit to the training data without necessarily overfitting.\n",
        "\n",
        "# High Comp Interval\n",
        "The high complexity model achieved perfect training accuracy (100%) but did not improve on test accuracy, suggesting it might be overfitting the training data.\n",
        "\n",
        "#Overfitting:\n",
        "The high complexity model, despite its perfect score on training data, fails to improve on unseen data, suggesting it's too tailored to the training examples.\n",
        "\n",
        "# Underfitting:\n",
        "The low complexity model, while not explicitly underperforming, could be seen as too simplistic as it might not capture all the patterns in the data.\n",
        "\n",
        "# Best Model Parameters\n",
        "Using 5-fold cross-validation and grid search, the analysis concluded that a simple decision tree (max_depth=1) with gini criterion is optimal, suggesting that beyond a certain point, adding complexity doesn't yield better generalization.\n",
        "\n",
        "# Confidence Interval\n",
        "The confidence interval calculated for the best model's accuracy (approximately 69.6% to 71.6%) provides an estimate of where we expect the model's true accuracy to lie 95% of the time, giving us confidence in its stability.\n",
        "\n",
        "# Medium vs Best\n",
        "Between the medium complexity and the best (simple) model, the simple model is favored for its comparable performance in test scenarios but with far less complexity. This suggests that for this dataset, a simpler model achieves effective generalization without the need for intricate decision-making processes. The choice leans towards the simpler model, emphasizing efficiency and generalizability. Despite the higher training accuracy of the medium complexity model, its similar test performance to the simpler model indicates no significant benefit from the added complexity."
      ],
      "metadata": {
        "id": "BA60qGxZR_VF"
      }
    }
  ]
}